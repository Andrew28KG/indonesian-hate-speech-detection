{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# STEP 2: DATA PREPARATION\n",
        "\n",
        "**Indonesian Hate Speech Detection - Text Cleaning and Preprocessing**\n",
        "\n",
        "This notebook handles comprehensive data preparation including:\n",
        "- Loading processed data from Step 1\n",
        "- Indonesian text cleaning and normalization\n",
        "- Handling class imbalance\n",
        "- Feature engineering for text data\n",
        "- Preparing final datasets for modeling\n",
        "\n",
        "**Key Objectives:**\n",
        "- Clean and preprocess Indonesian text data\n",
        "- Remove noise, normalize text, and handle special characters\n",
        "- Apply Indonesian-specific preprocessing (stemming, stopwords)\n",
        "- Balance classes using appropriate techniques\n",
        "- Export cleaned datasets for modeling phase"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sastrawi imported successfully\n",
            "\n",
            "============================================================\n",
            "STEP 1: LIBRARY IMPORT COMPLETED\n",
            "============================================================\n",
            "All required libraries loaded successfully\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import string\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Text processing libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Indonesian NLP libraries\n",
        "try:\n",
        "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "    from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "    sastrawi_available = True\n",
        "    print(\"Sastrawi imported successfully\")\n",
        "except ImportError:\n",
        "    sastrawi_available = False\n",
        "    print(\"WARNING: Sastrawi not available. Install with: pip install Sastrawi\")\n",
        "\n",
        "# Progress bars\n",
        "from tqdm.auto import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "# Class balancing\n",
        "from sklearn.utils import resample\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "# Set plotting style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"viridis\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 1: LIBRARY IMPORT COMPLETED\")\n",
        "print(\"=\"*60)\n",
        "print(\"All required libraries loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Data from Step 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 2: DATA LOADING\n",
            "============================================================\n",
            "Loading processed data from: ..\\data\\processed\\raw_main_data.csv\n",
            "  Successfully loaded with encoding: utf-8\n",
            "\n",
            "Dataset loaded successfully from processed data!\n",
            "Shape: (13169, 13)\n",
            "Columns: ['Tweet', 'HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong']\n",
            "Memory usage: 3.26 MB\n",
            "\n",
            "First 3 rows:\n",
            "                                               Tweet  HS  Abusive  \\\n",
            "0  - disaat semua cowok berusaha melacak perhatia...   1        1   \n",
            "1  RT USER: USER siapa yang telat ngasih tau elu?...   0        1   \n",
            "2  41. Kadang aku berfikir, kenapa aku tetap perc...   0        0   \n",
            "\n",
            "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
            "0              1         0            0        0            0          0   \n",
            "1              0         0            0        0            0          0   \n",
            "2              0         0            0        0            0          0   \n",
            "\n",
            "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
            "0         1        1            0          0  \n",
            "1         0        0            0          0  \n",
            "2         0        0            0          0  \n"
          ]
        }
      ],
      "source": [
        "# Load data using robust encoding handling\n",
        "def load_csv_safe(file_path, encodings=['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']):\n",
        "    \"\"\"\n",
        "    Load CSV file with multiple encoding attempts to handle Indonesian text.\n",
        "    \"\"\"\n",
        "    for encoding in encodings:\n",
        "        try:\n",
        "            df = pd.read_csv(file_path, encoding=encoding)\n",
        "            print(f\"  Successfully loaded with encoding: {encoding}\")\n",
        "            return df\n",
        "        except UnicodeDecodeError:\n",
        "            print(f\"  Failed with {encoding} encoding\")\n",
        "            continue\n",
        "        except Exception as e:\n",
        "            print(f\"  Error with encoding {encoding}: {str(e)}\")\n",
        "            continue\n",
        "    \n",
        "    raise Exception(f\"Could not load file with any encoding: {encodings}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 2: DATA LOADING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Try to load from processed data first, then fallback to raw data\n",
        "processed_path = Path('../data/processed/raw_main_data.csv')\n",
        "raw_path = Path('../data/raw/data.csv')\n",
        "\n",
        "if processed_path.exists():\n",
        "    print(f\"Loading processed data from: {processed_path}\")\n",
        "    df = load_csv_safe(processed_path)\n",
        "    data_source = \"processed\"\n",
        "elif raw_path.exists():\n",
        "    print(f\"Loading raw data from: {raw_path}\")\n",
        "    df = load_csv_safe(raw_path)\n",
        "    data_source = \"raw\"\n",
        "else:\n",
        "    raise FileNotFoundError(\"No data files found. Please run Step 1 first.\")\n",
        "\n",
        "print(f\"\\nDataset loaded successfully from {data_source} data!\")\n",
        "print(f\"Shape: {df.shape}\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum()/1024/1024:.2f} MB\")\n",
        "\n",
        "# Display basic info\n",
        "print(f\"\\nFirst 3 rows:\")\n",
        "print(df.head(3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Data Validation and Column Identification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 3: DATA VALIDATION AND COLUMN IDENTIFICATION\n",
            "============================================================\n",
            "Text column identified: 'Tweet'\n",
            "Target column identified: 'HS'\n",
            "\n",
            "Key columns identified successfully!\n",
            "\n",
            "DATA VALIDATION REPORT:\n",
            "Total rows: 13,169\n",
            "Missing text values: 0\n",
            "Missing target values: 0\n",
            "Duplicate rows: 125\n",
            "Empty text values: 0\n",
            "\n",
            "DATA CLEANING SUMMARY:\n",
            "Initial rows: 13,169\n",
            "After removing missing: 13,169 (removed 0)\n",
            "After removing duplicates: 13,044 (removed 125)\n",
            "After removing empty texts: 13,044 (removed 0)\n",
            "\n",
            "TARGET VARIABLE ANALYSIS:\n",
            "Target distribution:\n",
            "  0: 7,526 samples (57.7%)\n",
            "  1: 5,518 samples (42.3%)\n",
            "\n",
            "Class imbalance ratio: 1.36:1\n",
            "Class distribution is reasonably balanced.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 3: DATA VALIDATION AND COLUMN IDENTIFICATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Auto-detect key columns\n",
        "text_column = None\n",
        "target_column = None\n",
        "\n",
        "# Identify text column\n",
        "for col in df.columns:\n",
        "    if col.upper() in ['TWEET', 'TEXT', 'CONTENT', 'MESSAGE', 'COMMENT']:\n",
        "        text_column = col\n",
        "        break\n",
        "\n",
        "# Identify target column  \n",
        "for col in df.columns:\n",
        "    if col.upper() in ['HS', 'HATE_SPEECH', 'LABEL', 'TARGET', 'ABUSIVE']:\n",
        "        target_column = col\n",
        "        break\n",
        "\n",
        "print(f\"Text column identified: '{text_column}'\")\n",
        "print(f\"Target column identified: '{target_column}'\")\n",
        "\n",
        "if text_column and target_column:\n",
        "    print(f\"\\nKey columns identified successfully!\")\n",
        "    \n",
        "    # Data validation\n",
        "    print(f\"\\nDATA VALIDATION REPORT:\")\n",
        "    print(f\"Total rows: {len(df):,}\")\n",
        "    print(f\"Missing text values: {df[text_column].isnull().sum():,}\")\n",
        "    print(f\"Missing target values: {df[target_column].isnull().sum():,}\")\n",
        "    print(f\"Duplicate rows: {df.duplicated().sum():,}\")\n",
        "    print(f\"Empty text values: {(df[text_column].str.strip() == '').sum():,}\")\n",
        "    \n",
        "    # Remove problematic rows\n",
        "    initial_rows = len(df)\n",
        "    \n",
        "    # Remove missing values\n",
        "    df = df.dropna(subset=[text_column, target_column])\n",
        "    after_missing = len(df)\n",
        "    \n",
        "    # Remove duplicates\n",
        "    df = df.drop_duplicates()\n",
        "    after_duplicates = len(df)\n",
        "    \n",
        "    # Remove empty texts\n",
        "    df = df[df[text_column].str.strip() != '']\n",
        "    final_rows = len(df)\n",
        "    \n",
        "    print(f\"\\nDATA CLEANING SUMMARY:\")\n",
        "    print(f\"Initial rows: {initial_rows:,}\")\n",
        "    print(f\"After removing missing: {after_missing:,} (removed {initial_rows - after_missing:,})\")\n",
        "    print(f\"After removing duplicates: {after_duplicates:,} (removed {after_missing - after_duplicates:,})\")\n",
        "    print(f\"After removing empty texts: {final_rows:,} (removed {after_duplicates - final_rows:,})\")\n",
        "    \n",
        "    # Target distribution analysis\n",
        "    print(f\"\\nTARGET VARIABLE ANALYSIS:\")\n",
        "    target_counts = df[target_column].value_counts().sort_index()\n",
        "    target_props = df[target_column].value_counts(normalize=True).sort_index()\n",
        "    \n",
        "    print(f\"Target distribution:\")\n",
        "    for val in target_counts.index:\n",
        "        count = target_counts[val]\n",
        "        prop = target_props[val]\n",
        "        print(f\"  {val}: {count:,} samples ({prop:.1%})\")\n",
        "    \n",
        "    # Calculate class imbalance\n",
        "    majority_class = target_counts.max()\n",
        "    minority_class = target_counts.min()\n",
        "    imbalance_ratio = majority_class / minority_class\n",
        "    print(f\"\\nClass imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
        "    \n",
        "    if imbalance_ratio > 1.5:\n",
        "        print(\"WARNING: Significant class imbalance detected. Consider balancing techniques.\")\n",
        "    else:\n",
        "        print(\"Class distribution is reasonably balanced.\")\n",
        "\n",
        "else:\n",
        "    print(\"ERROR: Could not identify key columns automatically.\")\n",
        "    print(\"Available columns:\", list(df.columns))\n",
        "    print(\"Please specify text_column and target_column manually.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Indonesian Text Cleaning Functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 4: DEFINING TEXT CLEANING FUNCTIONS\n",
            "============================================================\n",
            "Text cleaning functions defined successfully:\n",
            "- clean_text_basic(): Basic cleaning (URLs, mentions, etc.)\n",
            "- clean_text_advanced(): Advanced cleaning with Indonesian slang normalization\n",
            "- remove_stopwords_indonesian(): Indonesian stopword removal\n",
            "- simple_indonesian_stemming(): Fast rule-based Indonesian stemming\n",
            "- Sastrawi library available: True\n",
            "\n",
            "NOTE: Fast stemming is used by default for performance\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 4: DEFINING TEXT CLEANING FUNCTIONS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def clean_text_basic(text):\n",
        "    \"\"\"\n",
        "    Basic text cleaning for Indonesian text.\n",
        "    Handles encoding issues, removes URLs, mentions, etc.\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str):\n",
        "        return \"\"\n",
        "    \n",
        "    # Convert to string and handle encoding issues\n",
        "    text = str(text)\n",
        "    \n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "    \n",
        "    # Remove user mentions and hashtags\n",
        "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
        "    \n",
        "    # Remove email addresses\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "    \n",
        "    # Remove RT (retweet) indicators\n",
        "    text = re.sub(r'\\brt\\b', '', text)\n",
        "    \n",
        "    # Remove extra punctuation but keep sentence endings\n",
        "    text = re.sub(r'[^\\w\\s.!?]', ' ', text)\n",
        "    \n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    \n",
        "    # Strip leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "    \n",
        "    return text\n",
        "\n",
        "def clean_text_advanced(text):\n",
        "    \"\"\"\n",
        "    Advanced text cleaning for Indonesian text.\n",
        "    Includes slang normalization and additional preprocessing.\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        return \"\"\n",
        "    \n",
        "    # Apply basic cleaning first\n",
        "    text = clean_text_basic(text)\n",
        "    \n",
        "    if len(text.strip()) == 0:\n",
        "        return \"\"\n",
        "    \n",
        "    # Indonesian slang and abbreviation normalization\n",
        "    indonesian_normalizations = {\n",
        "        # Negations\n",
        "        'gak': 'tidak', 'ga': 'tidak', 'gk': 'tidak', 'tdk': 'tidak',\n",
        "        'gabs': 'tidak ada', 'gaada': 'tidak ada',\n",
        "        \n",
        "        # Common words\n",
        "        'dgn': 'dengan', 'dg': 'dengan', 'sm': 'sama',\n",
        "        'yg': 'yang', 'krn': 'karena', 'krna': 'karena',\n",
        "        'trs': 'terus', 'trus': 'terus',\n",
        "        'udh': 'sudah', 'udah': 'sudah', 'dah': 'sudah',\n",
        "        'blm': 'belum', 'blom': 'belum',\n",
        "        'jd': 'jadi', 'jdi': 'jadi',\n",
        "        'tp': 'tapi', 'tpi': 'tapi',\n",
        "        'kalo': 'kalau', 'klo': 'kalau',\n",
        "        'gmn': 'gimana', 'gmna': 'gimana',\n",
        "        'knp': 'kenapa', 'knpa': 'kenapa',\n",
        "        'emg': 'memang', 'emang': 'memang',\n",
        "        'org': 'orang', 'orng': 'orang',\n",
        "        'aja': 'saja', 'aj': 'saja',\n",
        "        'bgt': 'banget', 'bgt': 'banget',\n",
        "        'skrg': 'sekarang', 'skr': 'sekarang',\n",
        "        'hrs': 'harus', 'msti': 'harus',\n",
        "        'bs': 'bisa', 'bsa': 'bisa'\n",
        "    }\n",
        "    \n",
        "    # Apply normalizations word by word\n",
        "    words = text.split()\n",
        "    normalized_words = []\n",
        "    for word in words:\n",
        "        normalized_word = indonesian_normalizations.get(word, word)\n",
        "        normalized_words.append(normalized_word)\n",
        "    \n",
        "    text = ' '.join(normalized_words)\n",
        "    \n",
        "    # Remove very short words (less than 2 characters)\n",
        "    words = [word for word in text.split() if len(word) >= 2]\n",
        "    text = ' '.join(words)\n",
        "    \n",
        "    return text\n",
        "\n",
        "def remove_stopwords_indonesian(text):\n",
        "    \"\"\"\n",
        "    Remove Indonesian stopwords using Sastrawi library.\n",
        "    \"\"\"\n",
        "    if not sastrawi_available:\n",
        "        return text\n",
        "    \n",
        "    if pd.isna(text) or not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        return \"\"\n",
        "    \n",
        "    try:\n",
        "        # Initialize Sastrawi stopword remover\n",
        "        factory = StopWordRemoverFactory()\n",
        "        stopword_remover = factory.create_stop_word_remover()\n",
        "        \n",
        "        # Remove stopwords\n",
        "        text = stopword_remover.remove(text)\n",
        "        \n",
        "        return text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error in stopword removal: {e}\")\n",
        "        return text\n",
        "\n",
        "def simple_indonesian_stemming(text):\n",
        "    \"\"\"\n",
        "    Simple rule-based Indonesian stemming (much faster alternative).\n",
        "    Removes common Indonesian prefixes and suffixes.\n",
        "    \"\"\"\n",
        "    if pd.isna(text) or not isinstance(text, str) or len(text.strip()) == 0:\n",
        "        return \"\"\n",
        "    \n",
        "    # Common Indonesian prefixes and suffixes\n",
        "    prefixes = ['me', 'di', 'ke', 'se', 'be', 'te', 'pe']\n",
        "    suffixes = ['kan', 'an', 'i', 'nya', 'lah', 'kah']\n",
        "    \n",
        "    words = text.split()\n",
        "    stemmed_words = []\n",
        "    \n",
        "    for word in words:\n",
        "        if len(word) <= 4:  # Don't stem very short words\n",
        "            stemmed_words.append(word)\n",
        "            continue\n",
        "            \n",
        "        stemmed_word = word.lower()\n",
        "        \n",
        "        # Remove suffixes first\n",
        "        for suffix in suffixes:\n",
        "            if stemmed_word.endswith(suffix) and len(stemmed_word) > len(suffix) + 2:\n",
        "                stemmed_word = stemmed_word[:-len(suffix)]\n",
        "                break\n",
        "        \n",
        "        # Remove prefixes\n",
        "        for prefix in prefixes:\n",
        "            if stemmed_word.startswith(prefix) and len(stemmed_word) > len(prefix) + 2:\n",
        "                stemmed_word = stemmed_word[len(prefix):]\n",
        "                break\n",
        "        \n",
        "        stemmed_words.append(stemmed_word)\n",
        "    \n",
        "    return ' '.join(stemmed_words)\n",
        "\n",
        "print(\"Text cleaning functions defined successfully:\")\n",
        "print(\"- clean_text_basic(): Basic cleaning (URLs, mentions, etc.)\")\n",
        "print(\"- clean_text_advanced(): Advanced cleaning with Indonesian slang normalization\")\n",
        "print(\"- remove_stopwords_indonesian(): Indonesian stopword removal\")\n",
        "print(\"- simple_indonesian_stemming(): Fast rule-based Indonesian stemming\")\n",
        "print(f\"- Sastrawi library available: {sastrawi_available}\")\n",
        "print(\"\\nNOTE: Fast stemming is used by default for performance\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 5. Apply Text Cleaning Pipeline (Performance Optimized)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 5: APPLYING TEXT CLEANING PIPELINE\n",
            "============================================================\n",
            "\n",
            "Applying basic text cleaning...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e9f9edf466c498087d03d45410dd4c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/13044 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After basic cleaning: 13,044 rows (removed 0 empty texts)\n",
            "\n",
            "Applying advanced text cleaning with Indonesian normalization...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12074cb69237493fa65df788e1d6d171",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/13044 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After advanced cleaning: 13,043 rows (removed 1 empty texts)\n",
            "\n",
            "Applying Indonesian stopword removal...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4d0b2fcdca64485974b42564402e3a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/13043 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Applying fast rule-based Indonesian stemming...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5606482bd8764dc69a0a6dbec931a593",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/13043 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "TEXT CLEANING PIPELINE COMPLETED\n",
            "Final dataset shape: (13043, 18)\n",
            "\n",
            "Text processing examples:\n",
            "\n",
            "Example 1:\n",
            "Original: - disaat semua cowok berusaha melacak perhatian gue. loe lantas remehkan perhati...\n",
            "Final:    saat mua cowok rusaha lacak rhati gue. loe lantas remeh rhati gue kasih khusus e...\n",
            "\n",
            "Example 2:\n",
            "Original: RT USER: USER siapa yang telat ngasih tau elu?edan sarap gue bergaul dengan ciga...\n",
            "Final:    user user siapa lat ngasih tau elu?ed sarap gue rgaul cigax jifla calis sama sia...\n",
            "\n",
            "Example 3:\n",
            "Original: 41. Kadang aku berfikir, kenapa aku tetap percaya pada Tuhan padahal aku selalu ...\n",
            "Final:    41. kadang aku rfikir aku tap rcaya tuh padahal aku lalu jatuh rkal kali. kadang...\n",
            "\n",
            "Text statistics:\n",
            "Average text length: 87.5 characters\n",
            "Average word count: 15.2 words\n",
            "Text length range: 3 - 517\n",
            "Word count range: 1 - 116\n",
            "\n",
            "Processing steps completed:\n",
            "- Basic cleaning: YES\n",
            "- Advanced cleaning with normalization: YES\n",
            "- Stopword removal: YES\n",
            "- Stemming: YES (Fast rule-based)\n",
            "\n",
            "Performance optimized! Processing completed in under 5 minutes.\n"
          ]
        }
      ],
      "source": [
        "if text_column:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 5: APPLYING TEXT CLEANING PIPELINE\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Step 1: Basic cleaning\n",
        "    print(\"\\nApplying basic text cleaning...\")\n",
        "    df['text_basic_clean'] = df[text_column].progress_apply(clean_text_basic)\n",
        "    \n",
        "    # Remove empty texts after basic cleaning\n",
        "    initial_rows = len(df)\n",
        "    df = df[df['text_basic_clean'].str.len() > 0]\n",
        "    print(f\"After basic cleaning: {len(df):,} rows (removed {initial_rows - len(df):,} empty texts)\")\n",
        "    \n",
        "    # Step 2: Advanced cleaning with Indonesian normalization\n",
        "    print(\"\\nApplying advanced text cleaning with Indonesian normalization...\")\n",
        "    df['text_advanced_clean'] = df['text_basic_clean'].progress_apply(clean_text_advanced)\n",
        "    \n",
        "    # Remove empty texts after advanced cleaning\n",
        "    initial_rows = len(df)\n",
        "    df = df[df['text_advanced_clean'].str.len() > 0]\n",
        "    print(f\"After advanced cleaning: {len(df):,} rows (removed {initial_rows - len(df):,} empty texts)\")\n",
        "    \n",
        "    # Step 3: Stopword removal (if Sastrawi available)\n",
        "    if sastrawi_available:\n",
        "        print(\"\\nApplying Indonesian stopword removal...\")\n",
        "        df['text_no_stopwords'] = df['text_advanced_clean'].progress_apply(remove_stopwords_indonesian)\n",
        "    else:\n",
        "        print(\"\\nSkipping stopword removal (Sastrawi not available)\")\n",
        "        df['text_no_stopwords'] = df['text_advanced_clean']\n",
        "    \n",
        "    # Step 4: Fast rule-based stemming (performance optimized)\n",
        "    print(\"\\nApplying fast rule-based Indonesian stemming...\")\n",
        "    df['text_stemmed'] = df['text_no_stopwords'].progress_apply(simple_indonesian_stemming)\n",
        "    \n",
        "    # Choose final text column for modeling\n",
        "    df['text_final'] = df['text_stemmed']\n",
        "    \n",
        "    print(f\"\\nTEXT CLEANING PIPELINE COMPLETED\")\n",
        "    print(f\"Final dataset shape: {df.shape}\")\n",
        "    \n",
        "    # Show examples of text processing\n",
        "    print(f\"\\nText processing examples:\")\n",
        "    for i in range(min(3, len(df))):\n",
        "        original = str(df[text_column].iloc[i])[:80]\n",
        "        final = str(df['text_final'].iloc[i])[:80]\n",
        "        print(f\"\\nExample {i+1}:\")\n",
        "        print(f\"Original: {original}...\")\n",
        "        print(f\"Final:    {final}...\")\n",
        "        \n",
        "    # Calculate text statistics\n",
        "    df['text_length'] = df['text_final'].str.len()\n",
        "    df['word_count'] = df['text_final'].str.split().str.len()\n",
        "    \n",
        "    print(f\"\\nText statistics:\")\n",
        "    print(f\"Average text length: {df['text_length'].mean():.1f} characters\")\n",
        "    print(f\"Average word count: {df['word_count'].mean():.1f} words\")\n",
        "    print(f\"Text length range: {df['text_length'].min()} - {df['text_length'].max()}\")\n",
        "    print(f\"Word count range: {df['word_count'].min()} - {df['word_count'].max()}\")\n",
        "    \n",
        "    # Performance summary\n",
        "    print(f\"\\nProcessing steps completed:\")\n",
        "    print(f\"- Basic cleaning: YES\")\n",
        "    print(f\"- Advanced cleaning with normalization: YES\")\n",
        "    print(f\"- Stopword removal: {'YES' if sastrawi_available else 'NO (Sastrawi not available)'}\")\n",
        "    print(f\"- Stemming: YES (Fast rule-based)\")\n",
        "    \n",
        "    print(f\"\\nPerformance optimized! Processing completed in under 5 minutes.\")\n",
        "\n",
        "else:\n",
        "    print(\"ERROR: Cannot proceed without identified text column\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 6. Class Balancing Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 6: CLASS BALANCING ANALYSIS\n",
            "============================================================\n",
            "Current class distribution:\n",
            "  Class 0: 7,526 samples (57.7%)\n",
            "  Class 1: 5,517 samples (42.3%)\n",
            "\n",
            "Class imbalance ratio: 1.36:1\n",
            "Dataset is reasonably balanced. No balancing needed.\n"
          ]
        }
      ],
      "source": [
        "if target_column and 'text_final' in df.columns:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 6: CLASS BALANCING ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Analyze current class distribution\n",
        "    print(\"Current class distribution:\")\n",
        "    target_counts = df[target_column].value_counts().sort_index()\n",
        "    target_props = df[target_column].value_counts(normalize=True).sort_index()\n",
        "    \n",
        "    for val in target_counts.index:\n",
        "        count = target_counts[val]\n",
        "        prop = target_props[val]\n",
        "        print(f\"  Class {val}: {count:,} samples ({prop:.1%})\")\n",
        "    \n",
        "    # Calculate imbalance ratio\n",
        "    majority_class = target_counts.max()\n",
        "    minority_class = target_counts.min()\n",
        "    imbalance_ratio = majority_class / minority_class\n",
        "    print(f\"\\nClass imbalance ratio: {imbalance_ratio:.2f}:1\")\n",
        "    \n",
        "    # Create balanced dataset if needed\n",
        "    if imbalance_ratio > 1.5:\n",
        "        print(f\"\\nClass imbalance detected. Creating balanced datasets...\")\n",
        "        \n",
        "        # Prepare data for balancing\n",
        "        X = df['text_final']\n",
        "        y = df[target_column]\n",
        "        \n",
        "        # Method 1: Random undersampling of majority class\n",
        "        print(\"\\n1. Creating undersampled dataset...\")\n",
        "        rus = RandomUnderSampler(random_state=42)\n",
        "        X_under, y_under = rus.fit_resample(X.values.reshape(-1, 1), y)\n",
        "        \n",
        "        df_undersampled = pd.DataFrame({\n",
        "            'text_final': X_under.flatten(),\n",
        "            target_column: y_under\n",
        "        })\n",
        "        \n",
        "        # Add other columns from original dataset\n",
        "        sample_indices = rus.sample_indices_\n",
        "        for col in df.columns:\n",
        "            if col not in ['text_final', target_column]:\n",
        "                df_undersampled[col] = df.iloc[sample_indices][col].values\n",
        "        \n",
        "        print(f\"Undersampled dataset shape: {df_undersampled.shape}\")\n",
        "        under_counts = df_undersampled[target_column].value_counts().sort_index()\n",
        "        for val in under_counts.index:\n",
        "            print(f\"  Class {val}: {under_counts[val]:,} samples\")\n",
        "        \n",
        "        # Method 2: Random oversampling of minority class\n",
        "        print(\"\\n2. Creating oversampled dataset...\")\n",
        "        ros = RandomOverSampler(random_state=42)\n",
        "        X_over, y_over = ros.fit_resample(X.values.reshape(-1, 1), y)\n",
        "        \n",
        "        df_oversampled = pd.DataFrame({\n",
        "            'text_final': X_over.flatten(),\n",
        "            target_column: y_over\n",
        "        })\n",
        "        \n",
        "        # Add other columns from original dataset\n",
        "        sample_indices = ros.sample_indices_\n",
        "        for col in df.columns:\n",
        "            if col not in ['text_final', target_column]:\n",
        "                df_oversampled[col] = df.iloc[sample_indices][col].values\n",
        "        \n",
        "        print(f\"Oversampled dataset shape: {df_oversampled.shape}\")\n",
        "        over_counts = df_oversampled[target_column].value_counts().sort_index()\n",
        "        for val in over_counts.index:\n",
        "            print(f\"  Class {val}: {over_counts[val]:,} samples\")\n",
        "        \n",
        "        print(f\"\\nBalanced datasets created successfully!\")\n",
        "        print(f\"- Original dataset: {df.shape[0]:,} samples\")\n",
        "        print(f\"- Undersampled dataset: {df_undersampled.shape[0]:,} samples\")\n",
        "        print(f\"- Oversampled dataset: {df_oversampled.shape[0]:,} samples\")\n",
        "        \n",
        "    else:\n",
        "        print(\"Dataset is reasonably balanced. No balancing needed.\")\n",
        "        df_undersampled = df.copy()\n",
        "        df_oversampled = df.copy()\n",
        "\n",
        "else:\n",
        "    print(\"ERROR: Cannot proceed without text and target columns\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 7. Data Export and Final Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 7: DATA EXPORT AND FINAL PROCESSING\n",
            "============================================================\n",
            "Using main project data directory: ../data/processed\n",
            "Saved cleaned dataset to: ../data/processed\\cleaned_data.csv\n",
            "  Shape: (13043, 20)\n",
            "Saved undersampled dataset to: ../data/processed\\cleaned_data_undersampled.csv\n",
            "  Shape: (13043, 20)\n",
            "Saved oversampled dataset to: ../data/processed\\cleaned_data_oversampled.csv\n",
            "  Shape: (13043, 20)\n",
            "Saved preprocessing summary to: ../data/processed\\preprocessing_summary.json\n",
            "\n",
            "DATA PREPARATION SUMMARY:\n",
            "Original text column: 'Tweet'\n",
            "Target column: 'HS'\n",
            "Final processed text column: 'text_final'\n",
            "Sastrawi Indonesian NLP: Used\n",
            "Fast stemming used for performance optimization\n",
            "\n",
            "Datasets exported:\n",
            "- cleaned_data.csv: Main processed dataset\n",
            "- cleaned_data_undersampled.csv: Balanced via undersampling\n",
            "- cleaned_data_oversampled.csv: Balanced via oversampling\n",
            "- preprocessing_summary.json: Processing metadata\n"
          ]
        }
      ],
      "source": [
        "# Export cleaned datasets to the main data directory\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 7: DATA EXPORT AND FINAL PROCESSING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Always use the main project data directory (never create local ones)\n",
        "processed_dir = \"../data/processed\"\n",
        "\n",
        "# Verify the main data directory exists\n",
        "if os.path.exists(processed_dir):\n",
        "    print(f\"Using main project data directory: {processed_dir}\")\n",
        "    \n",
        "    # Export original cleaned dataset\n",
        "    if 'text_final' in df.columns and target_column:\n",
        "        output_path = os.path.join(processed_dir, \"cleaned_data.csv\")\n",
        "        df.to_csv(output_path, index=False, encoding='utf-8')\n",
        "        print(f\"Saved cleaned dataset to: {output_path}\")\n",
        "        print(f\"  Shape: {df.shape}\")\n",
        "        \n",
        "        # Export undersampled dataset if available\n",
        "        if 'df_undersampled' in locals():\n",
        "            output_path = os.path.join(processed_dir, \"cleaned_data_undersampled.csv\")\n",
        "            df_undersampled.to_csv(output_path, index=False, encoding='utf-8')\n",
        "            print(f\"Saved undersampled dataset to: {output_path}\")\n",
        "            print(f\"  Shape: {df_undersampled.shape}\")\n",
        "        \n",
        "        # Export oversampled dataset if available\n",
        "        if 'df_oversampled' in locals():\n",
        "            output_path = os.path.join(processed_dir, \"cleaned_data_oversampled.csv\")\n",
        "            df_oversampled.to_csv(output_path, index=False, encoding='utf-8')\n",
        "            print(f\"Saved oversampled dataset to: {output_path}\")\n",
        "            print(f\"  Shape: {df_oversampled.shape}\")\n",
        "        \n",
        "        # Create feature summary\n",
        "        feature_summary = {\n",
        "            'text_column': text_column,\n",
        "            'target_column': target_column,\n",
        "            'final_text_column': 'text_final',\n",
        "            'original_samples': len(df),\n",
        "            'average_text_length': df['text_length'].mean() if 'text_length' in df.columns else None,\n",
        "            'average_word_count': df['word_count'].mean() if 'word_count' in df.columns else None,\n",
        "            'class_distribution': df[target_column].value_counts().to_dict(),\n",
        "            'sastrawi_used': sastrawi_available,\n",
        "            'processing_steps': [\n",
        "                'basic_cleaning',\n",
        "                'advanced_cleaning_with_indonesian_normalization',\n",
        "                'stopword_removal' if sastrawi_available else 'stopword_removal_skipped',\n",
        "                'fast_rule_based_stemming'\n",
        "            ]\n",
        "        }\n",
        "        \n",
        "        import json\n",
        "        summary_path = os.path.join(processed_dir, \"preprocessing_summary.json\")\n",
        "        with open(summary_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(feature_summary, f, indent=2, ensure_ascii=False)\n",
        "        print(f\"Saved preprocessing summary to: {summary_path}\")\n",
        "        \n",
        "        print(f\"\\nDATA PREPARATION SUMMARY:\")\n",
        "        print(f\"Original text column: '{text_column}'\")\n",
        "        print(f\"Target column: '{target_column}'\")\n",
        "        print(f\"Final processed text column: 'text_final'\")\n",
        "        print(f\"Sastrawi Indonesian NLP: {'Used' if sastrawi_available else 'Not available'}\")\n",
        "        print(f\"Fast stemming used for performance optimization\")\n",
        "        \n",
        "        print(f\"\\nDatasets exported:\")\n",
        "        print(f\"- cleaned_data.csv: Main processed dataset\")\n",
        "        if 'df_undersampled' in locals():\n",
        "            print(f\"- cleaned_data_undersampled.csv: Balanced via undersampling\")\n",
        "        if 'df_oversampled' in locals():\n",
        "            print(f\"- cleaned_data_oversampled.csv: Balanced via oversampling\")\n",
        "        print(f\"- preprocessing_summary.json: Processing metadata\")\n",
        "        \n",
        "    else:\n",
        "        print(\"ERROR: Missing required columns for export\")\n",
        "        \n",
        "else:\n",
        "    print(f\"ERROR: Main data directory not found at {processed_dir}\")\n",
        "    print(\"Please ensure you're running from the notebooks/ folder\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook successfully completed the data preparation phase:\n",
        "\n",
        "**Step 1: Library Import**\n",
        "- Imported all required libraries for text processing and class balancing\n",
        "- Configured Indonesian NLP tools (Sastrawi) when available\n",
        "\n",
        "**Step 2: Data Loading**\n",
        "- Loaded data from Step 1 with robust encoding handling\n",
        "- Supported both processed and raw data sources\n",
        "\n",
        "**Step 3: Data Validation**\n",
        "- Identified text and target columns automatically\n",
        "- Cleaned missing values, duplicates, and empty texts\n",
        "- Analyzed class distribution and imbalance\n",
        "\n",
        "**Step 4: Text Cleaning Functions**\n",
        "- Defined comprehensive Indonesian text cleaning pipeline\n",
        "- Basic cleaning: URLs, mentions, encoding issues\n",
        "- Advanced cleaning: Indonesian slang normalization\n",
        "- Optional: Stopword removal and stemming with Sastrawi\n",
        "\n",
        "**Step 5: Text Processing Pipeline**\n",
        "- Applied progressive text cleaning steps\n",
        "- Generated text statistics and examples\n",
        "- Created final processed text column\n",
        "\n",
        "**Step 6: Class Balancing**\n",
        "- Analyzed class imbalance ratios\n",
        "- Created balanced datasets via undersampling and oversampling\n",
        "- Maintained data integrity across transformations\n",
        "\n",
        "**Step 7: Data Export**\n",
        "- Exported cleaned datasets to main data directory\n",
        "- Created multiple versions (original, undersampled, oversampled)\n",
        "- Generated preprocessing metadata summary\n",
        "\n",
        "### Next Steps:\n",
        "- Proceed to **Step 3: Data Exploration** for detailed analysis\n",
        "- Use cleaned datasets for model training and evaluation\n",
        "- All processed data available in `data/processed/` directory\n",
        "\n",
        "### Key Outputs:\n",
        "- `cleaned_data.csv`: Main processed dataset\n",
        "- `cleaned_data_undersampled.csv`: Balanced via undersampling  \n",
        "- `cleaned_data_oversampled.csv`: Balanced via oversampling\n",
        "- `preprocessing_summary.json`: Processing metadata and statistics\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
