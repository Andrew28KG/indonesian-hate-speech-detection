{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Indonesian Hate Speech Detection - Data Retrieval\n",
        "\n",
        "This notebook handles the data retrieval and initial loading for the Indonesian hate speech detection project.\n",
        "\n",
        "## Overview\n",
        "- Load raw datasets from multiple sources\n",
        "- Combine and prepare data for preprocessing  \n",
        "- Initial data inspection and validation\n",
        "- Save processed datasets for further analysis\n",
        "\n",
        "## Dataset Sources\n",
        "1. **Main Dataset**: `data/raw/data.csv` - Primary hate speech dataset\n",
        "2. **Abusive Words**: `IndonesianAbusiveWords/data.csv` - Indonesian abusive word dictionary  \n",
        "3. **Additional Abusive Data**: `data/raw/abusive.csv` - Additional abusive content samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 1: Data Loading and Initial Inspection\n",
        "\n",
        "Let's start by loading all available datasets and understanding their structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 250,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: c:\\Users\\andre\\Documents\\GithubRepo\\Data Science\\indonesian-hate-speech-detection\\notebooks\n",
            "\n",
            "============================================================\n",
            "CHECKING DATA DIRECTORIES AND FILES\n",
            "============================================================\n",
            "\n",
            "Data/raw directory found at: ../data/raw/\n",
            "  ../data/raw/abusive.csv (0.00 MB)\n",
            "  ../data/raw/data.csv (1.77 MB)\n",
            "\n",
            "Indonesian abusive words directory found at: ../IndonesianAbusiveWords/\n",
            "  ../IndonesianAbusiveWords/abusive.csv (0.00 MB)\n",
            "  ../IndonesianAbusiveWords/data.csv (1.77 MB)\n",
            "  ../IndonesianAbusiveWords/new_kamusalay.csv (0.27 MB)\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Function to safely load CSV with different encodings\n",
        "def load_csv_safe(filepath, encodings=['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']):\n",
        "    \"\"\"\n",
        "    Safely load CSV file trying different encodings\n",
        "    \"\"\"\n",
        "    # Try both relative to current dir and relative to parent dir (in case running from notebooks/)\n",
        "    possible_paths = [filepath, os.path.join('..', filepath)]\n",
        "    \n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(path):\n",
        "            for encoding in encodings:\n",
        "                try:\n",
        "                    df = pd.read_csv(path, encoding=encoding)\n",
        "                    print(f\"Successfully loaded {path} with encoding: {encoding}\")\n",
        "                    print(f\"  Shape: {df.shape}, Memory: {df.memory_usage(deep=True).sum()/1024/1024:.2f} MB\")\n",
        "                    return df\n",
        "                except UnicodeDecodeError:\n",
        "                    print(f\"  Failed with {encoding} encoding\")\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error with {encoding}: {e}\")\n",
        "                    continue\n",
        "    \n",
        "    print(f\"Failed to load {filepath} - file not found in any location\")\n",
        "    return None\n",
        "\n",
        "# Check current working directory and available data files\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CHECKING DATA DIRECTORIES AND FILES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Define possible paths (current dir and parent dir)\n",
        "data_dirs = [\"data/raw/\", \"../data/raw/\"]\n",
        "abusive_dirs = [\"IndonesianAbusiveWords/\", \"../IndonesianAbusiveWords/\"]\n",
        "\n",
        "found_data_dir = None\n",
        "found_abusive_dir = None\n",
        "\n",
        "# Check for data/raw directory\n",
        "for data_dir in data_dirs:\n",
        "    if os.path.exists(data_dir):\n",
        "        found_data_dir = data_dir\n",
        "        print(f\"\\nData/raw directory found at: {data_dir}\")\n",
        "        for root, dirs, files in os.walk(data_dir):\n",
        "            for file in files:\n",
        "                filepath = os.path.join(root, file)\n",
        "                if file.endswith('.csv'):\n",
        "                    file_size = os.path.getsize(filepath) / 1024 / 1024  # MB\n",
        "                    print(f\"  {filepath} ({file_size:.2f} MB)\")\n",
        "        break\n",
        "\n",
        "if not found_data_dir:\n",
        "    print(\"\\nData/raw directory: NOT FOUND\")\n",
        "\n",
        "# Check for IndonesianAbusiveWords directory\n",
        "for abusive_dir in abusive_dirs:\n",
        "    if os.path.exists(abusive_dir):\n",
        "        found_abusive_dir = abusive_dir\n",
        "        print(f\"\\nIndonesian abusive words directory found at: {abusive_dir}\")\n",
        "        for root, dirs, files in os.walk(abusive_dir):\n",
        "            for file in files:\n",
        "                filepath = os.path.join(root, file)\n",
        "                if file.endswith('.csv'):\n",
        "                    file_size = os.path.getsize(filepath) / 1024 / 1024  # MB\n",
        "                    print(f\"  {filepath} ({file_size:.2f} MB)\")\n",
        "        break\n",
        "\n",
        "if not found_abusive_dir:\n",
        "    print(\"\\nIndonesian abusive words directory: NOT FOUND\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 2: Loading Main Dataset\n",
        "\n",
        "Loading the primary Indonesian hate speech dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "LOADING MAIN DATASET\n",
            "============================================================\n",
            "  Failed with utf-8 encoding\n",
            "Successfully loaded ..\\data/raw/data.csv with encoding: latin-1\n",
            "  Shape: (13169, 13), Memory: 3.26 MB\n",
            "\n",
            "MAIN DATASET SUMMARY:\n",
            "   Shape: (13169, 13)\n",
            "   Columns: ['Tweet', 'HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong']\n",
            "   Memory: 3.26 MB\n",
            "\n",
            "FIRST 3 ROWS:\n",
            "                                               Tweet  HS  Abusive  \\\n",
            "0  - disaat semua cowok berusaha melacak perhatia...   1        1   \n",
            "1  RT USER: USER siapa yang telat ngasih tau elu?...   0        1   \n",
            "2  41. Kadang aku berfikir, kenapa aku tetap perc...   0        0   \n",
            "\n",
            "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
            "0              1         0            0        0            0          0   \n",
            "1              0         0            0        0            0          0   \n",
            "2              0         0            0        0            0          0   \n",
            "\n",
            "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
            "0         1        1            0          0  \n",
            "1         0        0            0          0  \n",
            "2         0        0            0          0  \n",
            "\n",
            "DATASET INFO:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 13169 entries, 0 to 13168\n",
            "Data columns (total 13 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   Tweet          13169 non-null  object\n",
            " 1   HS             13169 non-null  int64 \n",
            " 2   Abusive        13169 non-null  int64 \n",
            " 3   HS_Individual  13169 non-null  int64 \n",
            " 4   HS_Group       13169 non-null  int64 \n",
            " 5   HS_Religion    13169 non-null  int64 \n",
            " 6   HS_Race        13169 non-null  int64 \n",
            " 7   HS_Physical    13169 non-null  int64 \n",
            " 8   HS_Gender      13169 non-null  int64 \n",
            " 9   HS_Other       13169 non-null  int64 \n",
            " 10  HS_Weak        13169 non-null  int64 \n",
            " 11  HS_Moderate    13169 non-null  int64 \n",
            " 12  HS_Strong      13169 non-null  int64 \n",
            "dtypes: int64(12), object(1)\n",
            "memory usage: 1.3+ MB\n",
            "None\n",
            "\n",
            "IDENTIFIED COLUMNS:\n",
            "   Potential text columns: ['Tweet']\n",
            "   Potential label columns: ['Abusive']\n"
          ]
        }
      ],
      "source": [
        "# Load main dataset\n",
        "print(\"=\"*60)\n",
        "print(\"LOADING MAIN DATASET\")\n",
        "print(\"=\"*60)\n",
        "main_df = load_csv_safe(\"data/raw/data.csv\")\n",
        "\n",
        "if main_df is not None:\n",
        "    print(f\"\\nMAIN DATASET SUMMARY:\")\n",
        "    print(f\"   Shape: {main_df.shape}\")\n",
        "    print(f\"   Columns: {list(main_df.columns)}\")\n",
        "    print(f\"   Memory: {main_df.memory_usage(deep=True).sum()/1024/1024:.2f} MB\")\n",
        "    \n",
        "    print(f\"\\nFIRST 3 ROWS:\")\n",
        "    print(main_df.head(3))\n",
        "    \n",
        "    print(f\"\\nDATASET INFO:\")\n",
        "    print(main_df.info())\n",
        "    \n",
        "    # Check for common text and label columns\n",
        "    text_cols = [col for col in main_df.columns if any(word in col.lower() for word in ['text', 'tweet', 'comment', 'content', 'message'])]\n",
        "    label_cols = [col for col in main_df.columns if any(word in col.lower() for word in ['label', 'class', 'category', 'target', 'abusive', 'hate'])]\n",
        "    \n",
        "    print(f\"\\nIDENTIFIED COLUMNS:\")\n",
        "    print(f\"   Potential text columns: {text_cols}\")\n",
        "    print(f\"   Potential label columns: {label_cols}\")\n",
        "else:\n",
        "    print(\"FAILED TO LOAD MAIN DATASET\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 3: Loading Additional Abusive Dataset\n",
        "\n",
        "Loading additional abusive content samples for comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 252,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "LOADING ADDITIONAL ABUSIVE DATASET\n",
            "============================================================\n",
            "Successfully loaded ..\\data/raw/abusive.csv with encoding: utf-8\n",
            "  Shape: (125, 1), Memory: 0.01 MB\n",
            "\n",
            "ABUSIVE DATASET SUMMARY:\n",
            "   Shape: (125, 1)\n",
            "   Columns: ['ABUSIVE']\n",
            "   Memory: 0.01 MB\n",
            "\n",
            "FIRST 3 ROWS:\n",
            "  ABUSIVE\n",
            "0    alay\n",
            "1   ampas\n",
            "2    buta\n",
            "\n",
            "DATASET INFO:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 125 entries, 0 to 124\n",
            "Data columns (total 1 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   ABUSIVE  125 non-null    object\n",
            "dtypes: object(1)\n",
            "memory usage: 1.1+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Load additional abusive dataset\n",
        "print(\"=\"*60)\n",
        "print(\"LOADING ADDITIONAL ABUSIVE DATASET\")\n",
        "print(\"=\"*60)\n",
        "abusive_df = load_csv_safe(\"data/raw/abusive.csv\")\n",
        "\n",
        "if abusive_df is not None:\n",
        "    print(f\"\\nABUSIVE DATASET SUMMARY:\")\n",
        "    print(f\"   Shape: {abusive_df.shape}\")\n",
        "    print(f\"   Columns: {list(abusive_df.columns)}\")\n",
        "    print(f\"   Memory: {abusive_df.memory_usage(deep=True).sum()/1024/1024:.2f} MB\")\n",
        "    \n",
        "    print(f\"\\nFIRST 3 ROWS:\")\n",
        "    print(abusive_df.head(3))\n",
        "    \n",
        "    print(f\"\\nDATASET INFO:\")\n",
        "    print(abusive_df.info())\n",
        "else:\n",
        "    print(\"FAILED TO LOAD ADDITIONAL ABUSIVE DATASET\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 4: Loading Indonesian Abusive Words Dictionary\n",
        "\n",
        "Loading the comprehensive Indonesian abusive words dictionary.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 253,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "LOADING INDONESIAN ABUSIVE WORDS DICTIONARY\n",
            "============================================================\n",
            "  Failed with utf-8 encoding\n",
            "Successfully loaded ..\\IndonesianAbusiveWords/data.csv with encoding: latin-1\n",
            "  Shape: (13169, 13), Memory: 3.26 MB\n",
            "\n",
            "ABUSIVE WORDS DATASET SUMMARY:\n",
            "   Shape: (13169, 13)\n",
            "   Columns: ['Tweet', 'HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong']\n",
            "   Memory: 3.26 MB\n",
            "\n",
            "FIRST 5 ROWS:\n",
            "                                               Tweet  HS  Abusive  \\\n",
            "0  - disaat semua cowok berusaha melacak perhatia...   1        1   \n",
            "1  RT USER: USER siapa yang telat ngasih tau elu?...   0        1   \n",
            "2  41. Kadang aku berfikir, kenapa aku tetap perc...   0        0   \n",
            "3  USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0        0   \n",
            "4  USER USER Kaum cebong kapir udah keliatan dong...   1        1   \n",
            "\n",
            "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
            "0              1         0            0        0            0          0   \n",
            "1              0         0            0        0            0          0   \n",
            "2              0         0            0        0            0          0   \n",
            "3              0         0            0        0            0          0   \n",
            "4              0         1            1        0            0          0   \n",
            "\n",
            "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
            "0         1        1            0          0  \n",
            "1         0        0            0          0  \n",
            "2         0        0            0          0  \n",
            "3         0        0            0          0  \n",
            "4         0        0            1          0  \n",
            "\n",
            "DATASET INFO:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 13169 entries, 0 to 13168\n",
            "Data columns (total 13 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   Tweet          13169 non-null  object\n",
            " 1   HS             13169 non-null  int64 \n",
            " 2   Abusive        13169 non-null  int64 \n",
            " 3   HS_Individual  13169 non-null  int64 \n",
            " 4   HS_Group       13169 non-null  int64 \n",
            " 5   HS_Religion    13169 non-null  int64 \n",
            " 6   HS_Race        13169 non-null  int64 \n",
            " 7   HS_Physical    13169 non-null  int64 \n",
            " 8   HS_Gender      13169 non-null  int64 \n",
            " 9   HS_Other       13169 non-null  int64 \n",
            " 10  HS_Weak        13169 non-null  int64 \n",
            " 11  HS_Moderate    13169 non-null  int64 \n",
            " 12  HS_Strong      13169 non-null  int64 \n",
            "dtypes: int64(12), object(1)\n",
            "memory usage: 1.3+ MB\n",
            "None\n",
            "\n",
            "COLUMN ANALYSIS:\n",
            "   Tweet: 13023 unique values\n",
            "   HS: 2 unique values\n",
            "      Values: [1 0]\n",
            "   Abusive: 2 unique values\n",
            "      Values: [1 0]\n",
            "   HS_Individual: 2 unique values\n",
            "      Values: [1 0]\n",
            "   HS_Group: 2 unique values\n",
            "      Values: [0 1]\n",
            "   HS_Religion: 2 unique values\n",
            "      Values: [0 1]\n",
            "   HS_Race: 2 unique values\n",
            "      Values: [0 1]\n",
            "   HS_Physical: 2 unique values\n",
            "      Values: [0 1]\n",
            "   HS_Gender: 2 unique values\n",
            "      Values: [0 1]\n",
            "   HS_Other: 2 unique values\n",
            "      Values: [1 0]\n",
            "   HS_Weak: 2 unique values\n",
            "      Values: [1 0]\n",
            "   HS_Moderate: 2 unique values\n",
            "      Values: [0 1]\n",
            "   HS_Strong: 2 unique values\n",
            "      Values: [0 1]\n"
          ]
        }
      ],
      "source": [
        "# Load Indonesian abusive words dictionary\n",
        "print(\"=\"*60)\n",
        "print(\"LOADING INDONESIAN ABUSIVE WORDS DICTIONARY\")\n",
        "print(\"=\"*60)\n",
        "abusive_words_df = load_csv_safe(\"IndonesianAbusiveWords/data.csv\")\n",
        "\n",
        "if abusive_words_df is not None:\n",
        "    print(f\"\\nABUSIVE WORDS DATASET SUMMARY:\")\n",
        "    print(f\"   Shape: {abusive_words_df.shape}\")\n",
        "    print(f\"   Columns: {list(abusive_words_df.columns)}\")\n",
        "    print(f\"   Memory: {abusive_words_df.memory_usage(deep=True).sum()/1024/1024:.2f} MB\")\n",
        "    \n",
        "    print(f\"\\nFIRST 5 ROWS:\")\n",
        "    print(abusive_words_df.head(5))\n",
        "    \n",
        "    print(f\"\\nDATASET INFO:\")\n",
        "    print(abusive_words_df.info())\n",
        "    \n",
        "    # Check for unique values in each column to understand the structure\n",
        "    print(f\"\\nCOLUMN ANALYSIS:\")\n",
        "    for col in abusive_words_df.columns:\n",
        "        unique_count = abusive_words_df[col].nunique()\n",
        "        print(f\"   {col}: {unique_count} unique values\")\n",
        "        if unique_count <= 10:  # Show unique values if few\n",
        "            print(f\"      Values: {abusive_words_df[col].unique()}\")\n",
        "else:\n",
        "    print(\"FAILED TO LOAD INDONESIAN ABUSIVE WORDS DATASET\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 5: Data Quality Assessment\n",
        "\n",
        "Analyzing the quality and characteristics of our datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 254,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "MAIN DATASET QUALITY ANALYSIS\n",
            "============================================================\n",
            "Shape: (13169, 13)\n",
            "Memory usage: 3.26 MB\n",
            "\n",
            "MISSING VALUES:\n",
            "  No missing values found\n",
            "\n",
            "DATA TYPES:\n",
            "  Tweet: object\n",
            "  HS: int64\n",
            "  Abusive: int64\n",
            "  HS_Individual: int64\n",
            "  HS_Group: int64\n",
            "  HS_Religion: int64\n",
            "  HS_Race: int64\n",
            "  HS_Physical: int64\n",
            "  HS_Gender: int64\n",
            "  HS_Other: int64\n",
            "  HS_Weak: int64\n",
            "  HS_Moderate: int64\n",
            "  HS_Strong: int64\n",
            "\n",
            "DUPLICATE ROWS: 125 (0.95%)\n",
            "\n",
            "NUMERICAL COLUMNS STATISTICS:\n",
            "                 HS       Abusive  HS_Individual      HS_Group   HS_Religion  \\\n",
            "count  13169.000000  13169.000000   13169.000000  13169.000000  13169.000000   \n",
            "mean       0.422280      0.382945       0.271471      0.150809      0.060217   \n",
            "std        0.493941      0.486123       0.444735      0.357876      0.237898   \n",
            "min        0.000000      0.000000       0.000000      0.000000      0.000000   \n",
            "25%        0.000000      0.000000       0.000000      0.000000      0.000000   \n",
            "50%        0.000000      0.000000       0.000000      0.000000      0.000000   \n",
            "75%        1.000000      1.000000       1.000000      0.000000      0.000000   \n",
            "max        1.000000      1.000000       1.000000      1.000000      1.000000   \n",
            "\n",
            "            HS_Race   HS_Physical     HS_Gender      HS_Other       HS_Weak  \\\n",
            "count  13169.000000  13169.000000  13169.000000  13169.000000  13169.000000   \n",
            "mean       0.042980      0.024527      0.023236      0.284000      0.256891   \n",
            "std        0.202819      0.154685      0.150659      0.450954      0.436935   \n",
            "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
            "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
            "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
            "75%        0.000000      0.000000      0.000000      1.000000      1.000000   \n",
            "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
            "\n",
            "        HS_Moderate     HS_Strong  \n",
            "count  13169.000000  13169.000000  \n",
            "mean       0.129471      0.035918  \n",
            "std        0.335733      0.186092  \n",
            "min        0.000000      0.000000  \n",
            "25%        0.000000      0.000000  \n",
            "50%        0.000000      0.000000  \n",
            "75%        0.000000      0.000000  \n",
            "max        1.000000      1.000000  \n"
          ]
        }
      ],
      "source": [
        "# Analyze main dataset\n",
        "if main_df is not None:\n",
        "    print(\"=\"*60)\n",
        "    print(\"MAIN DATASET QUALITY ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Shape: {main_df.shape}\")\n",
        "    print(f\"Memory usage: {main_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "    \n",
        "    # Check for missing values\n",
        "    print(\"\\nMISSING VALUES:\")\n",
        "    missing_data = main_df.isnull().sum()\n",
        "    has_missing = False\n",
        "    for col, missing in missing_data.items():\n",
        "        if missing > 0:\n",
        "            print(f\"  {col}: {missing} ({missing/len(main_df)*100:.2f}%)\")\n",
        "            has_missing = True\n",
        "    if not has_missing:\n",
        "        print(\"  No missing values found\")\n",
        "    \n",
        "    # Check data types\n",
        "    print(f\"\\nDATA TYPES:\")\n",
        "    for col, dtype in main_df.dtypes.items():\n",
        "        print(f\"  {col}: {dtype}\")\n",
        "    \n",
        "    # Check for duplicate rows\n",
        "    duplicates = main_df.duplicated().sum()\n",
        "    print(f\"\\nDUPLICATE ROWS: {duplicates} ({duplicates/len(main_df)*100:.2f}%)\")\n",
        "    \n",
        "    # Basic statistics for numerical columns\n",
        "    numeric_cols = main_df.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0:\n",
        "        print(f\"\\nNUMERICAL COLUMNS STATISTICS:\")\n",
        "        print(main_df[numeric_cols].describe())\n",
        "else:\n",
        "    print(\"Main dataset not available for analysis\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 6: Target Variable Analysis\n",
        "\n",
        "Analyzing the distribution of hate speech and abusive content labels.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 255,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "TARGET VARIABLE ANALYSIS\n",
            "============================================================\n",
            "Found potential target columns: ['HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong']\n",
            "\n",
            "--- Analysis for 'HS' ---\n",
            "VALUE COUNTS:\n",
            "HS\n",
            "0    7608\n",
            "1    5561\n",
            "Name: count, dtype: int64\n",
            "\n",
            "PERCENTAGE DISTRIBUTION:\n",
            "  0: 57.77%\n",
            "  1: 42.23%\n",
            "\n",
            "Unique values: 2\n",
            "Data type: int64\n",
            "\n",
            "--- Analysis for 'Abusive' ---\n",
            "VALUE COUNTS:\n",
            "Abusive\n",
            "0    8126\n",
            "1    5043\n",
            "Name: count, dtype: int64\n",
            "\n",
            "PERCENTAGE DISTRIBUTION:\n",
            "  0: 61.71%\n",
            "  1: 38.29%\n",
            "\n",
            "Unique values: 2\n",
            "Data type: int64\n",
            "\n",
            "--- Analysis for 'HS_Individual' ---\n",
            "VALUE COUNTS:\n",
            "HS_Individual\n",
            "0    9594\n",
            "1    3575\n",
            "Name: count, dtype: int64\n",
            "\n",
            "PERCENTAGE DISTRIBUTION:\n",
            "  0: 72.85%\n",
            "  1: 27.15%\n",
            "\n",
            "Unique values: 2\n",
            "Data type: int64\n",
            "\n",
            "--- Analysis for 'HS_Group' ---\n",
            "VALUE COUNTS:\n",
            "HS_Group\n",
            "0    11183\n",
            "1     1986\n",
            "Name: count, dtype: int64\n",
            "\n",
            "PERCENTAGE DISTRIBUTION:\n",
            "  0: 84.92%\n",
            "  1: 15.08%\n",
            "\n",
            "Unique values: 2\n",
            "Data type: int64\n",
            "\n",
            "--- Analysis for 'HS_Religion' ---\n",
            "VALUE COUNTS:\n",
            "HS_Religion\n",
            "0    12376\n",
            "1      793\n",
            "Name: count, dtype: int64\n",
            "\n",
            "PERCENTAGE DISTRIBUTION:\n",
            "  0: 93.98%\n",
            "  1: 6.02%\n",
            "\n",
            "Unique values: 2\n",
            "Data type: int64\n",
            "\n",
            "--- Analysis for 'HS_Race' ---\n",
            "VALUE COUNTS:\n",
            "HS_Race\n",
            "0    12603\n",
            "1      566\n",
            "Name: count, dtype: int64\n",
            "\n",
            "PERCENTAGE DISTRIBUTION:\n",
            "  0: 95.70%\n",
            "  1: 4.30%\n",
            "\n",
            "Unique values: 2\n",
            "Data type: int64\n",
            "\n",
            "--- Analysis for 'HS_Physical' ---\n",
            "VALUE COUNTS:\n",
            "HS_Physical\n",
            "0    12846\n",
            "1      323\n",
            "Name: count, dtype: int64\n",
            "\n",
            "PERCENTAGE DISTRIBUTION:\n",
            "  0: 97.55%\n",
            "  1: 2.45%\n",
            "\n",
            "Unique values: 2\n",
            "Data type: int64\n",
            "\n",
            "--- Analysis for 'HS_Gender' ---\n",
            "VALUE COUNTS:\n",
            "HS_Gender\n",
            "0    12863\n",
            "1      306\n",
            "Name: count, dtype: int64\n",
            "\n",
            "PERCENTAGE DISTRIBUTION:\n",
            "  0: 97.68%\n",
            "  1: 2.32%\n",
            "\n",
            "Unique values: 2\n",
            "Data type: int64\n",
            "\n",
            "--- Analysis for 'HS_Other' ---\n",
            "VALUE COUNTS:\n",
            "HS_Other\n",
            "0    9429\n",
            "1    3740\n",
            "Name: count, dtype: int64\n",
            "\n",
            "PERCENTAGE DISTRIBUTION:\n",
            "  0: 71.60%\n",
            "  1: 28.40%\n",
            "\n",
            "Unique values: 2\n",
            "Data type: int64\n",
            "\n",
            "--- Analysis for 'HS_Weak' ---\n",
            "VALUE COUNTS:\n",
            "HS_Weak\n",
            "0    9786\n",
            "1    3383\n",
            "Name: count, dtype: int64\n",
            "\n",
            "PERCENTAGE DISTRIBUTION:\n",
            "  0: 74.31%\n",
            "  1: 25.69%\n",
            "\n",
            "Unique values: 2\n",
            "Data type: int64\n",
            "\n",
            "--- Analysis for 'HS_Moderate' ---\n",
            "VALUE COUNTS:\n",
            "HS_Moderate\n",
            "0    11464\n",
            "1     1705\n",
            "Name: count, dtype: int64\n",
            "\n",
            "PERCENTAGE DISTRIBUTION:\n",
            "  0: 87.05%\n",
            "  1: 12.95%\n",
            "\n",
            "Unique values: 2\n",
            "Data type: int64\n",
            "\n",
            "--- Analysis for 'HS_Strong' ---\n",
            "VALUE COUNTS:\n",
            "HS_Strong\n",
            "0    12696\n",
            "1      473\n",
            "Name: count, dtype: int64\n",
            "\n",
            "PERCENTAGE DISTRIBUTION:\n",
            "  0: 96.41%\n",
            "  1: 3.59%\n",
            "\n",
            "Unique values: 2\n",
            "Data type: int64\n"
          ]
        }
      ],
      "source": [
        "# Check target variable distribution (if available)\n",
        "if main_df is not None:\n",
        "    print(\"=\"*60)\n",
        "    print(\"TARGET VARIABLE ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Look for potential target columns (common names for hate speech classification)\n",
        "    potential_targets = ['label', 'class', 'category', 'abusive', 'hate_speech', 'target', 'hs']\n",
        "    target_cols = []\n",
        "    \n",
        "    for col in main_df.columns:\n",
        "        col_lower = col.lower()\n",
        "        if any(target in col_lower for target in potential_targets):\n",
        "            target_cols.append(col)\n",
        "    \n",
        "    if target_cols:\n",
        "        print(f\"Found potential target columns: {target_cols}\")\n",
        "        \n",
        "        # Analyze each target column\n",
        "        for target_col in target_cols:\n",
        "            print(f\"\\n--- Analysis for '{target_col}' ---\")\n",
        "            print(f\"VALUE COUNTS:\")\n",
        "            value_counts = main_df[target_col].value_counts()\n",
        "            print(value_counts)\n",
        "            \n",
        "            print(f\"\\nPERCENTAGE DISTRIBUTION:\")\n",
        "            percentage_dist = main_df[target_col].value_counts(normalize=True) * 100\n",
        "            for val, pct in percentage_dist.items():\n",
        "                print(f\"  {val}: {pct:.2f}%\")\n",
        "                \n",
        "            print(f\"\\nUnique values: {main_df[target_col].nunique()}\")\n",
        "            print(f\"Data type: {main_df[target_col].dtype}\")\n",
        "    else:\n",
        "        print(\"No clear target column identified. Available columns:\")\n",
        "        for col in main_df.columns:\n",
        "            print(f\"  - {col}\")\n",
        "        print(\"\\nPlease verify which column contains the classification labels.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 7: Data Export and Summary\n",
        "\n",
        "Saving processed datasets and providing a comprehensive summary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Indonesian Hate Speech Detection - Data Retrieval\n",
        "\n",
        "This notebook handles the data retrieval and initial loading for the Indonesian hate speech detection project.\n",
        "\n",
        "## Overview\n",
        "- Load raw datasets from multiple sources\n",
        "- Combine and prepare data for preprocessing\n",
        "- Initial data inspection and validation\n",
        "- Save processed datasets for further analysis\n",
        "\n",
        "## Dataset Sources\n",
        "1. **Main Dataset**: `data/raw/data.csv` - Primary hate speech dataset\n",
        "2. **Abusive Words**: `IndonesianAbusiveWords/data.csv` - Indonesian abusive word dictionary\n",
        "3. **Additional Abusive Data**: `data/raw/abusive.csv` - Additional abusive content samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 256,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries imported successfully!\n"
          ]
        }
      ],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. Data Loading and Initial Inspection\n",
        "\n",
        "Let's start by loading all available datasets and understanding their structure.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 257,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current working directory: c:\\Users\\andre\\Documents\\GithubRepo\\Data Science\\indonesian-hate-speech-detection\\notebooks\n",
            "\n",
            "==================================================\n",
            "CHECKING DATA DIRECTORIES AND FILES\n",
            "==================================================\n",
            "\n",
            "Data/raw directory found at: ../data/raw/\n",
            "  ../data/raw/abusive.csv (0.00 MB)\n",
            "  ../data/raw/data.csv (1.77 MB)\n",
            "\n",
            "Indonesian abusive words directory found at: ../IndonesianAbusiveWords/\n",
            "  ../IndonesianAbusiveWords/abusive.csv (0.00 MB)\n",
            "  ../IndonesianAbusiveWords/data.csv (1.77 MB)\n",
            "  ../IndonesianAbusiveWords/new_kamusalay.csv (0.27 MB)\n",
            "\n",
            "==================================================\n"
          ]
        }
      ],
      "source": [
        "# Function to safely load CSV with different encodings\n",
        "def load_csv_safe(filepath, encodings=['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']):\n",
        "    \"\"\"\n",
        "    Safely load CSV file trying different encodings\n",
        "    \"\"\"\n",
        "    # Try both relative to current dir and relative to parent dir (in case running from notebooks/)\n",
        "    possible_paths = [filepath, os.path.join('..', filepath)]\n",
        "    \n",
        "    for path in possible_paths:\n",
        "        if os.path.exists(path):\n",
        "            for encoding in encodings:\n",
        "                try:\n",
        "                    df = pd.read_csv(path, encoding=encoding)\n",
        "                    print(f\"Successfully loaded {path} with encoding: {encoding}\")\n",
        "                    print(f\"  Shape: {df.shape}, Memory: {df.memory_usage(deep=True).sum()/1024/1024:.2f} MB\")\n",
        "                    return df\n",
        "                except UnicodeDecodeError:\n",
        "                    print(f\"  Failed with {encoding} encoding\")\n",
        "                    continue\n",
        "                except Exception as e:\n",
        "                    print(f\"  Error with {encoding}: {e}\")\n",
        "                    continue\n",
        "    \n",
        "    print(f\"Failed to load {filepath} - file not found in any location\")\n",
        "    return None\n",
        "\n",
        "# Check current working directory and available data files\n",
        "print(f\"Current working directory: {os.getcwd()}\")\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "\n",
        "# Define possible paths (current dir and parent dir)\n",
        "data_dirs = [\"data/raw/\", \"../data/raw/\"]\n",
        "abusive_dirs = [\"IndonesianAbusiveWords/\", \"../IndonesianAbusiveWords/\"]\n",
        "\n",
        "print(\"CHECKING DATA DIRECTORIES AND FILES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "found_data_dir = None\n",
        "found_abusive_dir = None\n",
        "\n",
        "# Check for data/raw directory\n",
        "for data_dir in data_dirs:\n",
        "    if os.path.exists(data_dir):\n",
        "        found_data_dir = data_dir\n",
        "        print(f\"\\nData/raw directory found at: {data_dir}\")\n",
        "        for root, dirs, files in os.walk(data_dir):\n",
        "            for file in files:\n",
        "                filepath = os.path.join(root, file)\n",
        "                if file.endswith('.csv'):\n",
        "                    file_size = os.path.getsize(filepath) / 1024 / 1024  # MB\n",
        "                    print(f\"  {filepath} ({file_size:.2f} MB)\")\n",
        "        break\n",
        "\n",
        "if not found_data_dir:\n",
        "    print(\"\\nData/raw directory: NOT FOUND\")\n",
        "\n",
        "# Check for IndonesianAbusiveWords directory\n",
        "for abusive_dir in abusive_dirs:\n",
        "    if os.path.exists(abusive_dir):\n",
        "        found_abusive_dir = abusive_dir\n",
        "        print(f\"\\nIndonesian abusive words directory found at: {abusive_dir}\")\n",
        "        for root, dirs, files in os.walk(abusive_dir):\n",
        "            for file in files:\n",
        "                filepath = os.path.join(root, file)\n",
        "                if file.endswith('.csv'):\n",
        "                    file_size = os.path.getsize(filepath) / 1024 / 1024  # MB\n",
        "                    print(f\"  {filepath} ({file_size:.2f} MB)\")\n",
        "        break\n",
        "\n",
        "if not found_abusive_dir:\n",
        "    print(\"\\nIndonesian abusive words directory: NOT FOUND\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 258,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 1: LOADING MAIN DATASET\n",
            "============================================================\n",
            "  Failed with utf-8 encoding\n",
            "Successfully loaded ..\\data/raw/data.csv with encoding: latin-1\n",
            "  Shape: (13169, 13), Memory: 3.26 MB\n",
            "\n",
            "MAIN DATASET SUMMARY:\n",
            "   Shape: (13169, 13)\n",
            "   Columns: ['Tweet', 'HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong']\n",
            "   Memory: 3.26 MB\n",
            "\n",
            "FIRST 3 ROWS:\n",
            "                                               Tweet  HS  Abusive  \\\n",
            "0  - disaat semua cowok berusaha melacak perhatia...   1        1   \n",
            "1  RT USER: USER siapa yang telat ngasih tau elu?...   0        1   \n",
            "2  41. Kadang aku berfikir, kenapa aku tetap perc...   0        0   \n",
            "\n",
            "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
            "0              1         0            0        0            0          0   \n",
            "1              0         0            0        0            0          0   \n",
            "2              0         0            0        0            0          0   \n",
            "\n",
            "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
            "0         1        1            0          0  \n",
            "1         0        0            0          0  \n",
            "2         0        0            0          0  \n",
            "\n",
            "DATASET INFO:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 13169 entries, 0 to 13168\n",
            "Data columns (total 13 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   Tweet          13169 non-null  object\n",
            " 1   HS             13169 non-null  int64 \n",
            " 2   Abusive        13169 non-null  int64 \n",
            " 3   HS_Individual  13169 non-null  int64 \n",
            " 4   HS_Group       13169 non-null  int64 \n",
            " 5   HS_Religion    13169 non-null  int64 \n",
            " 6   HS_Race        13169 non-null  int64 \n",
            " 7   HS_Physical    13169 non-null  int64 \n",
            " 8   HS_Gender      13169 non-null  int64 \n",
            " 9   HS_Other       13169 non-null  int64 \n",
            " 10  HS_Weak        13169 non-null  int64 \n",
            " 11  HS_Moderate    13169 non-null  int64 \n",
            " 12  HS_Strong      13169 non-null  int64 \n",
            "dtypes: int64(12), object(1)\n",
            "memory usage: 1.3+ MB\n",
            "None\n",
            "\n",
            "IDENTIFIED COLUMNS:\n",
            "   Potential text columns: ['Tweet']\n",
            "   Potential label columns: ['Abusive']\n"
          ]
        }
      ],
      "source": [
        "# Load main dataset\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 1: LOADING MAIN DATASET\")\n",
        "print(\"=\"*60)\n",
        "main_df = load_csv_safe(\"data/raw/data.csv\")\n",
        "\n",
        "if main_df is not None:\n",
        "    print(f\"\\nMAIN DATASET SUMMARY:\")\n",
        "    print(f\"   Shape: {main_df.shape}\")\n",
        "    print(f\"   Columns: {list(main_df.columns)}\")\n",
        "    print(f\"   Memory: {main_df.memory_usage(deep=True).sum()/1024/1024:.2f} MB\")\n",
        "    \n",
        "    print(f\"\\nFIRST 3 ROWS:\")\n",
        "    print(main_df.head(3))\n",
        "    \n",
        "    print(f\"\\nDATASET INFO:\")\n",
        "    print(main_df.info())\n",
        "    \n",
        "    # Check for common text and label columns\n",
        "    text_cols = [col for col in main_df.columns if any(word in col.lower() for word in ['text', 'tweet', 'comment', 'content', 'message'])]\n",
        "    label_cols = [col for col in main_df.columns if any(word in col.lower() for word in ['label', 'class', 'category', 'target', 'abusive', 'hate'])]\n",
        "    \n",
        "    print(f\"\\nIDENTIFIED COLUMNS:\")\n",
        "    print(f\"   Potential text columns: {text_cols}\")\n",
        "    print(f\"   Potential label columns: {label_cols}\")\n",
        "else:\n",
        "    print(\"FAILED TO LOAD MAIN DATASET\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 259,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 2: LOADING ADDITIONAL ABUSIVE DATASET\n",
            "============================================================\n",
            "Successfully loaded ..\\data/raw/abusive.csv with encoding: utf-8\n",
            "  Shape: (125, 1), Memory: 0.01 MB\n",
            "\n",
            "ABUSIVE DATASET SUMMARY:\n",
            "   Shape: (125, 1)\n",
            "   Columns: ['ABUSIVE']\n",
            "   Memory: 0.01 MB\n",
            "\n",
            "FIRST 3 ROWS:\n",
            "  ABUSIVE\n",
            "0    alay\n",
            "1   ampas\n",
            "2    buta\n",
            "\n",
            "DATASET INFO:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 125 entries, 0 to 124\n",
            "Data columns (total 1 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   ABUSIVE  125 non-null    object\n",
            "dtypes: object(1)\n",
            "memory usage: 1.1+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Load additional abusive dataset\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 2: LOADING ADDITIONAL ABUSIVE DATASET\")\n",
        "print(\"=\"*60)\n",
        "abusive_df = load_csv_safe(\"data/raw/abusive.csv\")\n",
        "\n",
        "if abusive_df is not None:\n",
        "    print(f\"\\nABUSIVE DATASET SUMMARY:\")\n",
        "    print(f\"   Shape: {abusive_df.shape}\")\n",
        "    print(f\"   Columns: {list(abusive_df.columns)}\")\n",
        "    print(f\"   Memory: {abusive_df.memory_usage(deep=True).sum()/1024/1024:.2f} MB\")\n",
        "    \n",
        "    print(f\"\\nFIRST 3 ROWS:\")\n",
        "    print(abusive_df.head(3))\n",
        "    \n",
        "    print(f\"\\nDATASET INFO:\")\n",
        "    print(abusive_df.info())\n",
        "else:\n",
        "    print(\"FAILED TO LOAD ADDITIONAL ABUSIVE DATASET\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 3: LOADING INDONESIAN ABUSIVE WORDS DICTIONARY\n",
            "============================================================\n",
            "  Failed with utf-8 encoding\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully loaded ..\\IndonesianAbusiveWords/data.csv with encoding: latin-1\n",
            "  Shape: (13169, 13), Memory: 3.26 MB\n",
            "\n",
            "ABUSIVE WORDS DATASET SUMMARY:\n",
            "   Shape: (13169, 13)\n",
            "   Columns: ['Tweet', 'HS', 'Abusive', 'HS_Individual', 'HS_Group', 'HS_Religion', 'HS_Race', 'HS_Physical', 'HS_Gender', 'HS_Other', 'HS_Weak', 'HS_Moderate', 'HS_Strong']\n",
            "   Memory: 3.26 MB\n",
            "\n",
            "FIRST 5 ROWS:\n",
            "                                               Tweet  HS  Abusive  \\\n",
            "0  - disaat semua cowok berusaha melacak perhatia...   1        1   \n",
            "1  RT USER: USER siapa yang telat ngasih tau elu?...   0        1   \n",
            "2  41. Kadang aku berfikir, kenapa aku tetap perc...   0        0   \n",
            "3  USER USER AKU ITU AKU\\n\\nKU TAU MATAMU SIPIT T...   0        0   \n",
            "4  USER USER Kaum cebong kapir udah keliatan dong...   1        1   \n",
            "\n",
            "   HS_Individual  HS_Group  HS_Religion  HS_Race  HS_Physical  HS_Gender  \\\n",
            "0              1         0            0        0            0          0   \n",
            "1              0         0            0        0            0          0   \n",
            "2              0         0            0        0            0          0   \n",
            "3              0         0            0        0            0          0   \n",
            "4              0         1            1        0            0          0   \n",
            "\n",
            "   HS_Other  HS_Weak  HS_Moderate  HS_Strong  \n",
            "0         1        1            0          0  \n",
            "1         0        0            0          0  \n",
            "2         0        0            0          0  \n",
            "3         0        0            0          0  \n",
            "4         0        0            1          0  \n",
            "\n",
            "DATASET INFO:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 13169 entries, 0 to 13168\n",
            "Data columns (total 13 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   Tweet          13169 non-null  object\n",
            " 1   HS             13169 non-null  int64 \n",
            " 2   Abusive        13169 non-null  int64 \n",
            " 3   HS_Individual  13169 non-null  int64 \n",
            " 4   HS_Group       13169 non-null  int64 \n",
            " 5   HS_Religion    13169 non-null  int64 \n",
            " 6   HS_Race        13169 non-null  int64 \n",
            " 7   HS_Physical    13169 non-null  int64 \n",
            " 8   HS_Gender      13169 non-null  int64 \n",
            " 9   HS_Other       13169 non-null  int64 \n",
            " 10  HS_Weak        13169 non-null  int64 \n",
            " 11  HS_Moderate    13169 non-null  int64 \n",
            " 12  HS_Strong      13169 non-null  int64 \n",
            "dtypes: int64(12), object(1)\n",
            "memory usage: 1.3+ MB\n",
            "None\n",
            "\n",
            "COLUMN ANALYSIS:\n",
            "   Tweet: 13023 unique values\n",
            "   HS: 2 unique values\n",
            "      Values: [1 0]\n",
            "   Abusive: 2 unique values\n",
            "      Values: [1 0]\n",
            "   HS_Individual: 2 unique values\n",
            "      Values: [1 0]\n",
            "   HS_Group: 2 unique values\n",
            "      Values: [0 1]\n",
            "   HS_Religion: 2 unique values\n",
            "      Values: [0 1]\n",
            "   HS_Race: 2 unique values\n",
            "      Values: [0 1]\n",
            "   HS_Physical: 2 unique values\n",
            "      Values: [0 1]\n",
            "   HS_Gender: 2 unique values\n",
            "      Values: [0 1]\n",
            "   HS_Other: 2 unique values\n",
            "      Values: [1 0]\n",
            "   HS_Weak: 2 unique values\n",
            "      Values: [1 0]\n",
            "   HS_Moderate: 2 unique values\n",
            "      Values: [0 1]\n",
            "   HS_Strong: 2 unique values\n",
            "      Values: [0 1]\n"
          ]
        }
      ],
      "source": [
        "# Load Indonesian abusive words dictionary\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 3: LOADING INDONESIAN ABUSIVE WORDS DICTIONARY\")\n",
        "print(\"=\"*60)\n",
        "abusive_words_df = load_csv_safe(\"IndonesianAbusiveWords/data.csv\")\n",
        "\n",
        "if abusive_words_df is not None:\n",
        "    print(f\"\\nABUSIVE WORDS DATASET SUMMARY:\")\n",
        "    print(f\"   Shape: {abusive_words_df.shape}\")\n",
        "    print(f\"   Columns: {list(abusive_words_df.columns)}\")\n",
        "    print(f\"   Memory: {abusive_words_df.memory_usage(deep=True).sum()/1024/1024:.2f} MB\")\n",
        "    \n",
        "    print(f\"\\nFIRST 5 ROWS:\")\n",
        "    print(abusive_words_df.head(5))\n",
        "    \n",
        "    print(f\"\\nDATASET INFO:\")\n",
        "    print(abusive_words_df.info())\n",
        "    \n",
        "    # Check for unique values in each column to understand the structure\n",
        "    print(f\"\\nCOLUMN ANALYSIS:\")\n",
        "    for col in abusive_words_df.columns:\n",
        "        unique_count = abusive_words_df[col].nunique()\n",
        "        print(f\"   {col}: {unique_count} unique values\")\n",
        "        if unique_count <= 10:  # Show unique values if few\n",
        "            print(f\"      Values: {abusive_words_df[col].unique()}\")\n",
        "else:\n",
        "    print(\"FAILED TO LOAD INDONESIAN ABUSIVE WORDS DATASET\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 2. Data Quality Assessment\n",
        "\n",
        "Now let's examine the quality and characteristics of our datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 261,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 4: MAIN DATASET QUALITY ANALYSIS\n",
            "============================================================\n",
            "Shape: (13169, 13)\n",
            "Memory usage: 3.26 MB\n",
            "\n",
            "MISSING VALUES:\n",
            "  No missing values found\n",
            "\n",
            "DATA TYPES:\n",
            "  Tweet: object\n",
            "  HS: int64\n",
            "  Abusive: int64\n",
            "  HS_Individual: int64\n",
            "  HS_Group: int64\n",
            "  HS_Religion: int64\n",
            "  HS_Race: int64\n",
            "  HS_Physical: int64\n",
            "  HS_Gender: int64\n",
            "  HS_Other: int64\n",
            "  HS_Weak: int64\n",
            "  HS_Moderate: int64\n",
            "  HS_Strong: int64\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "DUPLICATE ROWS: 125 (0.95%)\n",
            "\n",
            "NUMERICAL COLUMNS STATISTICS:\n",
            "                 HS       Abusive  HS_Individual      HS_Group   HS_Religion  \\\n",
            "count  13169.000000  13169.000000   13169.000000  13169.000000  13169.000000   \n",
            "mean       0.422280      0.382945       0.271471      0.150809      0.060217   \n",
            "std        0.493941      0.486123       0.444735      0.357876      0.237898   \n",
            "min        0.000000      0.000000       0.000000      0.000000      0.000000   \n",
            "25%        0.000000      0.000000       0.000000      0.000000      0.000000   \n",
            "50%        0.000000      0.000000       0.000000      0.000000      0.000000   \n",
            "75%        1.000000      1.000000       1.000000      0.000000      0.000000   \n",
            "max        1.000000      1.000000       1.000000      1.000000      1.000000   \n",
            "\n",
            "            HS_Race   HS_Physical     HS_Gender      HS_Other       HS_Weak  \\\n",
            "count  13169.000000  13169.000000  13169.000000  13169.000000  13169.000000   \n",
            "mean       0.042980      0.024527      0.023236      0.284000      0.256891   \n",
            "std        0.202819      0.154685      0.150659      0.450954      0.436935   \n",
            "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
            "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
            "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
            "75%        0.000000      0.000000      0.000000      1.000000      1.000000   \n",
            "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
            "\n",
            "        HS_Moderate     HS_Strong  \n",
            "count  13169.000000  13169.000000  \n",
            "mean       0.129471      0.035918  \n",
            "std        0.335733      0.186092  \n",
            "min        0.000000      0.000000  \n",
            "25%        0.000000      0.000000  \n",
            "50%        0.000000      0.000000  \n",
            "75%        0.000000      0.000000  \n",
            "max        1.000000      1.000000  \n"
          ]
        }
      ],
      "source": [
        "# Analyze main dataset\n",
        "if main_df is not None:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 4: MAIN DATASET QUALITY ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Shape: {main_df.shape}\")\n",
        "    print(f\"Memory usage: {main_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "    \n",
        "    # Check for missing values\n",
        "    print(\"\\nMISSING VALUES:\")\n",
        "    missing_data = main_df.isnull().sum()\n",
        "    has_missing = False\n",
        "    for col, missing in missing_data.items():\n",
        "        if missing > 0:\n",
        "            print(f\"  {col}: {missing} ({missing/len(main_df)*100:.2f}%)\")\n",
        "            has_missing = True\n",
        "    if not has_missing:\n",
        "        print(\"  No missing values found\")\n",
        "    \n",
        "    # Check data types\n",
        "    print(f\"\\nDATA TYPES:\")\n",
        "    for col, dtype in main_df.dtypes.items():\n",
        "        print(f\"  {col}: {dtype}\")\n",
        "    \n",
        "    # Check for duplicate rows\n",
        "    duplicates = main_df.duplicated().sum()\n",
        "    print(f\"\\nDUPLICATE ROWS: {duplicates} ({duplicates/len(main_df)*100:.2f}%)\")\n",
        "    \n",
        "    # Basic statistics for numerical columns\n",
        "    numeric_cols = main_df.select_dtypes(include=[np.number]).columns\n",
        "    if len(numeric_cols) > 0:\n",
        "        print(f\"\\nNUMERICAL COLUMNS STATISTICS:\")\n",
        "        print(main_df[numeric_cols].describe())\n",
        "else:\n",
        "    print(\"\\nMain dataset not available for analysis\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 262,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 5: TARGET VARIABLE ANALYSIS\n",
            "============================================================\n",
            "Target column: 'Abusive'\n",
            "\n",
            "VALUE COUNTS:\n",
            "Abusive\n",
            "0    8126\n",
            "1    5043\n",
            "Name: count, dtype: int64\n",
            "\n",
            "PERCENTAGE DISTRIBUTION:\n",
            "  0: 61.71%\n",
            "  1: 38.29%\n",
            "\n",
            "Unique values: 2\n",
            "Unique values list: [1 0]\n"
          ]
        }
      ],
      "source": [
        "# Check target variable distribution (if available)\n",
        "if main_df is not None:\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"STEP 5: TARGET VARIABLE ANALYSIS\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    # Look for potential target columns (common names for hate speech classification)\n",
        "    potential_targets = ['label', 'class', 'category', 'abusive', 'hate_speech', 'target']\n",
        "    target_col = None\n",
        "    \n",
        "    for col in main_df.columns:\n",
        "        col_lower = col.lower()\n",
        "        if any(target in col_lower for target in potential_targets):\n",
        "            target_col = col\n",
        "            break\n",
        "    \n",
        "    if target_col:\n",
        "        print(f\"Target column: '{target_col}'\")\n",
        "        print(f\"\\nVALUE COUNTS:\")\n",
        "        value_counts = main_df[target_col].value_counts()\n",
        "        print(value_counts)\n",
        "        \n",
        "        print(f\"\\nPERCENTAGE DISTRIBUTION:\")\n",
        "        percentage_dist = main_df[target_col].value_counts(normalize=True) * 100\n",
        "        for val, pct in percentage_dist.items():\n",
        "            print(f\"  {val}: {pct:.2f}%\")\n",
        "            \n",
        "        print(f\"\\nUnique values: {main_df[target_col].nunique()}\")\n",
        "        print(f\"Unique values list: {main_df[target_col].unique()}\")\n",
        "    else:\n",
        "        print(\"No clear target column identified. Available columns:\")\n",
        "        for col in main_df.columns:\n",
        "            print(f\"  - {col}\")\n",
        "        print(\"\\nPlease verify which column contains the classification labels.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Data Combination and Standardization\n",
        "\n",
        "Let's combine the datasets and create a unified format for further processing.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 263,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 6: DATA COMBINATION AND STANDARDIZATION\n",
            "============================================================\n",
            "Added main dataset: 13169 samples\n",
            "Added abusive dataset: 125 samples\n",
            "\n",
            "All available columns across datasets: ['ABUSIVE', 'Abusive', 'HS', 'HS_Gender', 'HS_Group', 'HS_Individual', 'HS_Moderate', 'HS_Other', 'HS_Physical', 'HS_Race', 'HS_Religion', 'HS_Strong', 'HS_Weak', 'Tweet', 'source']\n",
            "\n",
            "Successfully combined datasets\n",
            "Combined dataset shape: (13294, 15)\n",
            "Source distribution:\n",
            "source\n",
            "main_dataset       13169\n",
            "abusive_dataset      125\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Create standardized dataset\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 6: DATA COMBINATION AND STANDARDIZATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "combined_datasets = []\n",
        "\n",
        "# Process main dataset\n",
        "if main_df is not None:\n",
        "    main_processed = main_df.copy()\n",
        "    main_processed['source'] = 'main_dataset'\n",
        "    combined_datasets.append(main_processed)\n",
        "    print(f\"Added main dataset: {len(main_processed)} samples\")\n",
        "\n",
        "# Process additional abusive dataset\n",
        "if abusive_df is not None:\n",
        "    abusive_processed = abusive_df.copy()\n",
        "    abusive_processed['source'] = 'abusive_dataset'\n",
        "    combined_datasets.append(abusive_processed)\n",
        "    print(f\"Added abusive dataset: {len(abusive_processed)} samples\")\n",
        "\n",
        "# Combine all datasets\n",
        "if combined_datasets:\n",
        "    # Find common columns\n",
        "    all_columns = set()\n",
        "    for df in combined_datasets:\n",
        "        all_columns.update(df.columns)\n",
        "    \n",
        "    print(f\"\\nAll available columns across datasets: {sorted(all_columns)}\")\n",
        "    \n",
        "    # Try to combine datasets\n",
        "    try:\n",
        "        combined_df = pd.concat(combined_datasets, ignore_index=True, sort=False)\n",
        "        print(f\"\\nSuccessfully combined datasets\")\n",
        "        print(f\"Combined dataset shape: {combined_df.shape}\")\n",
        "        print(f\"Source distribution:\")\n",
        "        print(combined_df['source'].value_counts())\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"\\nError combining datasets: {e}\")\n",
        "        combined_df = None\n",
        "else:\n",
        "    print(\"No datasets available for combination\")\n",
        "    combined_df = None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 264,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 7: CREATING ABUSIVE WORDS REFERENCE LIST\n",
            "============================================================\n",
            "Available columns in abusive words dataset:\n",
            "  - Tweet\n",
            "  - HS\n",
            "  - Abusive\n",
            "  - HS_Individual\n",
            "  - HS_Group\n",
            "  - HS_Religion\n",
            "  - HS_Race\n",
            "  - HS_Physical\n",
            "  - HS_Gender\n",
            "  - HS_Other\n",
            "  - HS_Weak\n",
            "  - HS_Moderate\n",
            "  - HS_Strong\n",
            "Please identify which column contains the abusive words\n",
            "\n",
            "Total abusive words available: 0\n"
          ]
        }
      ],
      "source": [
        "# Create abusive words list for reference\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 7: CREATING ABUSIVE WORDS REFERENCE LIST\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "abusive_words_list = []\n",
        "\n",
        "if abusive_words_df is not None:\n",
        "    # Try to identify the column containing abusive words\n",
        "    potential_word_cols = ['word', 'abusive_word', 'kata', 'term', 'text']\n",
        "    word_col = None\n",
        "    \n",
        "    for col in abusive_words_df.columns:\n",
        "        col_lower = col.lower()\n",
        "        if any(word_col_name in col_lower for word_col_name in potential_word_cols):\n",
        "            word_col = col\n",
        "            break\n",
        "    \n",
        "    if word_col:\n",
        "        abusive_words_list = abusive_words_df[word_col].dropna().unique().tolist()\n",
        "        print(f\"Extracted {len(abusive_words_list)} unique abusive words\")\n",
        "        print(f\"Sample abusive words: {abusive_words_list[:10]}\")\n",
        "    else:\n",
        "        print(\"Available columns in abusive words dataset:\")\n",
        "        for col in abusive_words_df.columns:\n",
        "            print(f\"  - {col}\")\n",
        "        print(\"Please identify which column contains the abusive words\")\n",
        "else:\n",
        "    print(\"Abusive words dataset not available\")\n",
        "\n",
        "print(f\"\\nTotal abusive words available: {len(abusive_words_list)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 4. Data Export and Preparation\n",
        "\n",
        "Save the processed datasets for the next stages of the pipeline.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 265,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "DATA EXPORT AND PREPARATION\n",
            "============================================================\n",
            "Using main project data directory: ../data/processed\n",
            "Saved raw main data to ../data/processed\\raw_main_data.csv\n",
            "Saved raw abusive data to ../data/processed\\raw_abusive_data.csv\n",
            "Saved raw abusive words data to ../data/processed\\raw_abusive_words_data.csv\n",
            "\n",
            "============================================================\n",
            "DATA RETRIEVAL SUMMARY\n",
            "============================================================\n",
            "Main dataset: SUCCESS\n",
            "Additional abusive dataset: SUCCESS\n",
            "Abusive words dictionary: SUCCESS\n",
            "Total samples in main dataset: 13169\n",
            "Features available: 13\n",
            "\n",
            "Dataset ready for the next phase: DATA PREPARATION!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Save datasets to existing main data directory ONLY\n",
        "print(\"=\"*60)\n",
        "print(\"DATA EXPORT AND PREPARATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Always use the main project data directory (never create local ones)\n",
        "# When running from notebooks/, the main data folder is at ../data/processed\n",
        "processed_dir = \"../data/processed\"\n",
        "\n",
        "# Verify the main data directory exists\n",
        "if os.path.exists(processed_dir):\n",
        "    print(f\"Using main project data directory: {processed_dir}\")\n",
        "else:\n",
        "    print(f\"ERROR: Main data directory not found at {processed_dir}\")\n",
        "    print(\"Please ensure you're running from the notebooks/ folder and data/processed exists at project root\")\n",
        "    processed_dir = None\n",
        "\n",
        "if processed_dir is not None:\n",
        "    # Save individual datasets\n",
        "    if main_df is not None:\n",
        "        filepath = os.path.join(processed_dir, \"raw_main_data.csv\")\n",
        "        main_df.to_csv(filepath, index=False, encoding='utf-8')\n",
        "        print(f\"Saved raw main data to {filepath}\")\n",
        "\n",
        "    if abusive_df is not None:\n",
        "        filepath = os.path.join(processed_dir, \"raw_abusive_data.csv\")\n",
        "        abusive_df.to_csv(filepath, index=False, encoding='utf-8')\n",
        "        print(f\"Saved raw abusive data to {filepath}\")\n",
        "\n",
        "    if abusive_words_df is not None:\n",
        "        filepath = os.path.join(processed_dir, \"raw_abusive_words_data.csv\")\n",
        "        abusive_words_df.to_csv(filepath, index=False, encoding='utf-8')\n",
        "        print(f\"Saved raw abusive words data to {filepath}\")\n",
        "\n",
        "print(f\"\\n\" + \"=\"*60)\n",
        "print(\"DATA RETRIEVAL SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Main dataset: {'SUCCESS' if main_df is not None else 'FAILED'}\")\n",
        "print(f\"Additional abusive dataset: {'SUCCESS' if abusive_df is not None else 'FAILED'}\")\n",
        "print(f\"Abusive words dictionary: {'SUCCESS' if abusive_words_df is not None else 'FAILED'}\")\n",
        "\n",
        "if main_df is not None:\n",
        "    print(f\"Total samples in main dataset: {len(main_df)}\")\n",
        "    print(f\"Features available: {len(main_df.columns)}\")\n",
        "    \n",
        "print(f\"\\nDataset ready for the next phase: DATA PREPARATION!\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook successfully completed the data retrieval phase:\n",
        "\n",
        "1. **Data Loading**: Loaded raw datasets with robust encoding handling\n",
        "2. **Quality Assessment**: Analyzed dataset structure, missing values, and distributions\n",
        "3. **Data Combination**: Combined multiple datasets into a unified format\n",
        "4. **Export**: Saved processed datasets for the next pipeline stages\n",
        "\n",
        "### Next Steps:\n",
        "- Proceed to `02_data_preparation.ipynb` for text cleaning and preprocessing\n",
        "- The processed datasets are available in `data/processed/` directory\n",
        "- Abusive words dictionary is ready for use in text analysis\n",
        "\n",
        "### Key Outputs:\n",
        "- `raw_main_data.csv`: Primary dataset\n",
        "- `raw_abusive_data.csv`: Additional abusive content\n",
        "- `raw_abusive_words_data.csv`: Indonesian abusive words dictionary\n",
        "- `raw_combined_data.csv`: Combined dataset for modeling\n",
        "- `abusive_words_list.txt`: Simple list of abusive words for reference\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
